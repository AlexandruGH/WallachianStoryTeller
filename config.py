# config.py - Model Router & Romanian-Aware Configuration
import re
import streamlit as st
from typing import List, Dict, Any
from deep_translator import GoogleTranslator
import os
import random
import requests
from models import NarrativeResponse

class Config:
    """Central configuration with Romanian-optimized models"""

    PRIMARY_API_MODEL = "dumitrescustefan/bloom-1b1-romanian"
    FALLBACK_API_MODELS = [
        "microsoft/DialoGPT-medium",
        "facebook/xglm-564M",
        "bigscience/bloom-560m"
    ]
    LOCAL_MODEL = "EleutherAI/gpt-neo-1.3B"

    IMAGE_MODEL = "stabilityai/stable-diffusion-2-1"
    IMAGE_INTERVAL = 3
    IMAGE_NEGATIVE = "modern, cartoon, anime, text, watermark, lowres, blurry, extra limbs"
    

    @staticmethod
    def make_intro_text(scale: int) -> str:
        # Propozi»õii istorice
        historical_sentences = [
            "Vlad »öepe»ô, domnitor al »öƒÉrii Rom√¢ne»ôti, a condus cu o m√¢nƒÉ de fier √Æntre anii 1448 »ôi 1476.",
            "CetƒÉ»õile de la poalele Carpa»õilor au fost √ÆntƒÉrite sub domnia lui, pentru a apƒÉra »õara de invazii.",
            "Metodele sale dure i-au adus at√¢t respect, c√¢t »ôi teamƒÉ √Æn r√¢ndul du»ômanilor »ôi al supu»ôilor.",
            "T√¢rgovi»ôte era inima puterii, locul unde deciziile puteau schimba soarta unui √Æntreg popor.",
            "Cronici vechi √Æl descriu ca pe un strateg necru»õƒÉtor, dar drept."
        ]

        # Propozi»õii legendare
        legendary_sentences = [
            "Se spune cƒÉ umbrele nop»õii prindeau via»õƒÉ √Æn prezen»õa lui.",
            "BƒÉtr√¢nii »ôoptesc cƒÉ putea chema creaturi ascunse √Æn bezna pƒÉdurilor.",
            "Legenda afirmƒÉ cƒÉ aerul se rƒÉcea brusc c√¢nd Vlad se m√¢nia.",
            "Unii credeau cƒÉ strƒÉvechi spirite √Æl protejau √Æn luptƒÉ.",
            "Se poveste»ôte cƒÉ s√¢ngele du»ômanilor √Æi √ÆntƒÉrea puterea."
        ]

        # TransformƒÉm scale √Æntr-un raport 0-1
        ratio = min(max(scale / 10.0, 0), 1)

        # NumƒÉr de propozi»õii istorice vs legendare
        num_hist = max(1, int((1 - ratio) * 3))  
        num_leg = max(1, int(ratio * 3))         

        # Alegem propozi»õii random
        chosen_hist = random.sample(historical_sentences, num_hist)
        chosen_leg = random.sample(legendary_sentences, num_leg)

        # Le mixƒÉm
        mixed_sentences = chosen_hist + chosen_leg
        random.shuffle(mixed_sentences)

        mixed_text = " ".join(mixed_sentences)

        # Intro narativ
        return (
        "»öara Rom√¢neascƒÉ, 1456. "
        + mixed_text
        + "\n\n"        
        )

    @staticmethod
    def build_dnd_prompt(story: List[Dict], character: Dict, legend_scale: int = 5) -> str:
        """Construie»ôte prompt pentru LLM folosind structuri de date simple (dict/list)"""
        from models import NarrativeResponse # Import local
        
        # Calcul raport legend vs istoric
        ratio = legend_scale / 10.0
        
        # Selectare prefix stil
        if ratio < 0.3:
            style_prefix = "Stil STRICT ISTORIC. FƒÉrƒÉ magie, fƒÉrƒÉ creaturi fantastice. "
        elif ratio > 0.7:
            style_prefix = "Stil LEGENDAR VAMPIRIC. Umbre, mister, folklore √Æntunecat. "
        else:
            style_prefix = "Stil echilibrat istoric »ôi legendar. "
        
        # Construire context narativ din ultimele replici
        context = "\n".join([f"{m['role'].upper()}: {m['text']}" for m in story[-4:]])
        
        # Construire Info caracter (AccesƒÉm prin CHEI de dic»õionar ['key'], nu prin punct .attr)
        # VerificƒÉm existen»õa cheilor cu .get() pentru siguran»õƒÉ
        char_health = character.get('health', 100)
        char_rep = character.get('reputation', 20)
        char_gold = character.get('gold', 0)
        char_loc = character.get('location', 'T√¢rgovi»ôte')
        
        power_desc = 'SLABƒÇ'
        if char_rep >= 60: power_desc = 'CRESCUTƒÇ'
        elif char_rep >= 30: power_desc = 'MEDIE'

        char_info = (
            f"\n\nSTATISTICI CRITICE: Via»õƒÉ={char_health} | "
            f"Reputa»õie={char_rep} | "
            f"Loca»õie={char_loc} | "
            f"Galbeni={char_gold} | "
            f"Puterea ta este {power_desc}\n"
        )
        
        # Restric»õii bazate pe reputa»õie
        restrictions = ""
        if char_rep < 20:
            restrictions = "JucƒÉtorul are reputa»õie FOARTE JOSƒÇ. Este tratat cu suspiciune. Nu poate intra √Æn audien»õƒÉ la boieri. "
        elif char_rep < 50:
            restrictions = "JucƒÉtorul are reputa»õie MEDIE. Poate interac»õiona cu negustori »ôi solda»õi, dar nu cu √Ænalta nobilime. "
        else:
            restrictions = "JucƒÉtorul are reputa»õie BUNƒÇ. Poate cere audien»õe, dar »öEPE»ò este INACCESIBIL direct fƒÉrƒÉ motiv √Æntemeiat. "
        
        # Schema JSON
        schema = NarrativeResponse.model_json_schema()
        
        # Construire instruc»õiuni finale
        instructions = (
            f"\n{style_prefix}{restrictions}"            
            "REGULI OBLIGATORII:\n"
            "- 'narrative': 2-3 propozi»õii, fƒÉrƒÉ gre»ôeli gramaticale, √Æn rom√¢nƒÉ medievalƒÉ\n"
            "- 'suggestions': ListƒÉ de EXACT 2-3 string-uri, fƒÉrƒÉ numere, fƒÉrƒÉ bullet points\n"
            "  EXEMPLU: [\"Cere audien»õƒÉ la curte.\", \"CautƒÉ informa»õii √Æn t√¢rg.\", \"Explorezi ad√¢ncul pƒÉdurii.\"]\n"
            "- RespectƒÉ gramatica: 'unei pƒÉsƒÉri', 'unor boieri', nu 'unui pƒÉsƒÉri'\n"
            "VLAD »öEPE»ò NU POATE FI √éNVINS ‚Äì orice tentativƒÉ = game_over instant\n"
            "Reputa»õia sub 20 = nu po»õi interac»õiona cu nobilii\n\n"
            f"RƒÉspunde STRICT √Æn format JSON conform schemei:\n"
            f"STRICT JSON SCHEMA:\n"
            f"```json\n{schema}\n```\n"
        )
    
        return context + char_info + instructions
    
    @staticmethod
    def generate_image_prompt_llm(text: str, location: str) -> str:
        """GenereazƒÉ prompt pentru Stable Diffusion folosind LLM"""
        token = os.getenv("GROQ_API_KEY") or st.secrets.get("GROQ_API_KEY", "")
        if not token:
            return Config.generate_image_prompt(text, location)

        system = (
            "You are an assistant that writes short, highly detailed prompts "
            "for Stable-Diffusion in English. "
            "Include: time of day, number of people, context, weather, camera angle, lighting style. "
            "Keep it under 200 characters. DO NOT include object names like 'candle'. "
            "Describe LIGHTING EFFECTS only: 'warm dim lighting', 'soft glow', 'moonlight'."
        )

        user = (
            f"Story fragment: {text}\n"
            f"Exact place: {location}\n"
            "Write one English Stable-Diffusion prompt with lighting effects, no objects."
        )

        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            "temperature": 0.75,
            "max_tokens": 60,
            "stream": False
        }

        try:
            r = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=15
            )
            r.raise_for_status()
            llm_prompt = r.json()["choices"][0]["message"]["content"].strip()
            llm_prompt = llm_prompt.replace('"', '')
            if llm_prompt.endswith('.'):
                llm_prompt = llm_prompt[:-1]
            
            # Prompt final cu descriere de iluminare, nu obiecte
            prompt = (
                f"Romanian medieval Wallachia 1456, Vlad Tepes era, atmospheric, "
                f"dark fantasy, {llm_prompt}, highly detailed, oil-on-canvas, "
                f"warm dim lighting, soft orange glow, deep shadows, 4k, vintage parchment look"
            )
            return prompt
        except Exception as e:
            print("LLM image-prompt failed:", e)
            # Fallback la metoda veche
            return Config.generate_image_prompt(text, location)
        
    @staticmethod
    def generate_image_prompt(text: str, location: str) -> str:
        # Extragem ultimele 3 propozi»õii sau primele 150 caractere
        short = " ".join(text.split(".")[-3:]).strip()
        if len(short) < 20:
            short = text[:150]

        def translate_to_english(text: str) -> str:
            """Traduce textul rom√¢nesc √Æn englezƒÉ pentru Stable Diffusion"""
            try:        
                return GoogleTranslator(source='ro', target='en').translate(text)
            except Exception as e:
                print(f"‚ö†Ô∏è Eroare traducere: {e}")
                return text  # Fallback la rom√¢nƒÉ
        # **TRADUCERE √Æn englezƒÉ**
        short_en = translate_to_english(short)
        location_en = translate_to_english(location)
        
        # Construim promptul √Æn englezƒÉ
        prompt = (
            f"Romanian medieval Wallachia 1456, Vlad Tepes era, atmospheric, "
            f"dark fantasy, {location_en}, {short_en}, highly detailed, oil-on-canvas, "
            f"warm candle-light, 4k, vintage parchment look"
        )
        
        return prompt[:195]
    
    @staticmethod
    def generate_image_prompt_llm(text: str, location: str) -> str:
        """
        Ask the SAME Groq endpoint we use for narration to write a short
        Stable-Diffusion prompt in English, grounded in the *exact* place
        and current narrative moment.
        """
        token = os.getenv("GROQ_API_KEY") or st.secrets.get("GROQ_API_KEY", "")
        if not token:
            # fallback to old method if somehow no key
            return Config.generate_image_prompt(text, location)

        system = (
            "You are an assistant that writes short, highly detailed prompts "
            "for Stable-Diffusion in English. "
            "Include: time of day, number of people in the image, context, weather, "
            "camera angle and any information usefull for image generation. Keep it under 200 characters."
        )

        user = (
            f"Story fragment: {text}\n"
            f"Exact place: {location}\n"
            "Write one English Stable-Diffusion prompt."
        )

        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            "temperature": 0.75,
            "max_tokens": 60,
            "stream": False
        }

        try:
            r = requests.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=15
            )
            r.raise_for_status()
            llm_prompt = r.json()["choices"][0]["message"]["content"].strip()
            # üîß CLEAN: remove quotes and trailing period
            llm_prompt = llm_prompt.replace('"', '')
            if llm_prompt.endswith('.'):
                llm_prompt = llm_prompt[:-1]
            prompt = (
            f"Romanian medieval Wallachia 1456, Vlad Tepes era, atmospheric, "
            f"dark fantasy, {llm_prompt}, highly detailed, oil-on-canvas, "
            f"warm dim lighting, deep shadows, 4k, vintage parchment look"
        )
            return prompt
        except Exception as e:
            print("LLM image-prompt failed:", e)
            # graceful fallback
            return Config.generate_image_prompt(text, location)


class ModelRouter:
    def __init__(self):
        self.api_models = Config.FALLBACK_API_MODELS.copy()
        self.api_models.insert(0, Config.PRIMARY_API_MODEL)
        self.current_api_index = 0

    def get_next_api_model(self) -> str:
        model = self.api_models[self.current_api_index]
        self.current_api_index = (self.current_api_index + 1) % len(self.api_models)
        return model