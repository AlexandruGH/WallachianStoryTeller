import streamlit as st
from PIL import Image, ImageDraw, ImageFont, ImageOps
from io import BytesIO
import requests
from typing import Optional, List
import threading
import time
import os
import hashlib
import base64
from config import Config

# Redis caching - lazy initialization
redis_client = None
REDIS_AVAILABLE = False

# Lista modelelor (prioritate descrescƒÉtoare)
IMAGE_MODELS: List[str] = [
    "stabilityai/stable-diffusion-xl-base-1.0"
]

# Thread-safe rotation pentru token-uri HF
_hf_token_index = 0
_hf_token_lock = threading.Lock()

def get_session_id():
    return st.session_state.get('session_id', 'UNKNOWN_SESSION')

def get_hf_tokens() -> List[str]:
    """Cite»ôte TOATE token-urile HF: HF_TOKEN, HF_TOKEN1, HF_TOKEN2, etc."""
    tokens = []
    token = os.getenv("HF_TOKEN")
    if token and token.strip():
        tokens.append(token.strip())
    
    i = 1
    while True:
        token = os.getenv(f"HF_TOKEN{i}")
        if token and token.strip():
            tokens.append(token.strip())
            i += 1
        else:
            break
    
    seen = set()
    unique_tokens = []
    for token in tokens:
        if token not in seen:
            seen.add(token)
            unique_tokens.append(token)
    
    return unique_tokens

def pil_to_bytes(img: Image.Image) -> bytes:
    buf = BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def generate_scene_image(text: str, is_initial: bool = False) -> Optional[bytes]:
    """
    GenereazƒÉ imagine folosind Hugging Face API cu caching Redis.
    DacƒÉ toate token-urile »ôi modelele e»ôueazƒÉ, returneazƒÉ None (fƒÉrƒÉ fallback).
    """
    # Lazy import of heavy ML library only when actually generating images
    try:
        from huggingface_hub import InferenceClient
    except ImportError:
        print("[IMAGE] ‚ùå HuggingFace library not available")
        return None

    session_id = get_session_id()
    location = st.session_state.character.get("location", "T√¢rgovi»ôte")

    # Create cache key based ONLY on text and location (before prompt generation)
    cache_key_data = {
        "text": text,
        "location": location
    }

    # Create hash for cache key
    cache_key_string = str(cache_key_data.items())
    cache_key = f"image:{hashlib.sha256(cache_key_string.encode()).hexdigest()}"

    # Lazy Redis initialization
    global REDIS_AVAILABLE, redis_client
    if redis_client is None:
        try:
            from upstash_redis import Redis
            redis_client = Redis(
                url=os.getenv("UPSTASH_REDIS_REST_URL"),
                token=os.getenv("UPSTASH_REDIS_REST_TOKEN")
            )
            REDIS_AVAILABLE = True
        except ImportError:
            redis_client = None
            REDIS_AVAILABLE = False

    # Check Redis cache first (before expensive LLM call)
    if REDIS_AVAILABLE and redis_client:
        try:
            cached_image = redis_client.get(cache_key)
            if cached_image:
                print(f"[SESSION {session_id}] üé® CACHE HIT - Imagine gƒÉsitƒÉ √Æn Redis (text+location)")
                return base64.b64decode(cached_image)
        except Exception as e:
            print(f"[SESSION {session_id}] ‚ö†Ô∏è Eroare citire cache Redis: {e}")

    # Generate prompt only if not cached
    prompt = Config.generate_image_prompt_llm(text, location)
    print(f"[SESSION {session_id}] ü§ñ Generat prompt LLM pentru imagine")

    tokens = get_hf_tokens()

    if not tokens:
        print(f"[SESSION {session_id}] üîí NU EXISTƒÇ TOKEN-URI HF - IMAGINEA NU SE GENEAZƒÇ")
        return None

    global _hf_token_index
    with _hf_token_lock:
        start_index = _hf_token_index
        _hf_token_index = (_hf_token_index + 1) % len(tokens)

    for i in range(len(tokens)):
        token_index = (start_index + i) % len(tokens)
        token = tokens[token_index]

        print(f"[SESSION {session_id}] üé® √éNCERC TOKEN {token_index + 1}")

        for model in IMAGE_MODELS:
            try:
                client = InferenceClient(
                    provider="nscale",
                    api_key=token,
                    timeout=120
                )

                print(f"[SESSION {session_id}] ‚úÖ Token {token_index + 1}, Model {model}")

                with st.spinner("üé® Artistul medieval lucreazƒÉ..."):
                    pil_img = client.text_to_image(
                        prompt,
                        model=model,
                        negative_prompt=Config.IMAGE_NEGATIVE,
                        num_inference_steps=30,
                        guidance_scale=7.5,
                    )

                if pil_img:
                    print(f"[SESSION {session_id}] ‚úÖ IMAGINE GENERATƒÇ (Token {token_index + 1})")
                    image_bytes = pil_to_bytes(pil_img)

                    # Cache the image in Redis (permanent cache)
                    if REDIS_AVAILABLE and redis_client:
                        try:
                            encoded_image = base64.b64encode(image_bytes).decode('utf-8')
                            redis_client.set(cache_key, encoded_image)  # Permanent cache, no expiration
                            print(f"[SESSION {session_id}] üíæ Imagine salvatƒÉ √Æn Redis cache (permanent)")
                        except Exception as e:
                            print(f"[SESSION {session_id}] ‚ö†Ô∏è Eroare salvare cache Redis: {e}")

                    return image_bytes

            except Exception as e:
                print(f"[SESSION {session_id}] ‚ùå E»òEC IMAGINE (Token {token_index + 1}, Model {model}): {e}")
                continue

    # TOATE TOKEN-URILE »òI MODELELE AU E»òUAT
    print(f"[SESSION {session_id}] ‚ùå TOATE TOKEN-URILE »òI MODELELE DE IMAGINE AU E»òUAT")
    #st.warning("‚ö†Ô∏è Generarea imaginilor este temporar indisponibilƒÉ.")
    return None
