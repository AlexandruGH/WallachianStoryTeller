import os
import sys
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import requests
import threading
import time
import random
import re
import json
from streamlit.runtime.scriptrunner import add_script_run_ctx
from pydantic import ValidationError # â­• FIX: Added explicit Pydantic ValidationError import

from models import InventoryItem, NarrativeResponse

if os.name == 'nt':
    os.environ["HF_HOME"] = "D:/huggingface_cache"
    os.makedirs("D:/huggingface_cache", exist_ok=True)


SYSTEM_PROMPT = (
    "EÈ™ti Naratorul TÄƒrÃ¢mului Valah Ã®n veacul al XV-lea, Ã®n zilele domniei lui Vlad ÈšepeÈ™ (DrÄƒculea). "
    "Tonul tÄƒu este medieval romÃ¢nesc: grav, aspru, veridic È™i autentic, folosind expresii arhaice È™i un vocabular variat specific epocii (ex: zÄƒbavÄƒ, hrisov, pÃ¢rcÄƒlab, ienicer, podoabÄƒ, tÄƒinuind). "
    "Nu folosi obiecte, noÈ›iuni sau emoÈ›ii moderne (ex: puÈ™ti, singurÄƒtate, fricÄƒ excesivÄƒ) È™i evitÄƒ orice meta-comentariu. "

    "Mecanica narativÄƒ È™i coerenÈ›a: "
    "1. **Anti-RepetiÈ›ie StrictÄƒ:** VarieazÄƒ structura propoziÈ›iilor, descrierile (vÃ¢nt/umbre) È™i verbele. Nu repeta descrieri similare Ã®n douÄƒ rÄƒspunsuri consecutive. "
    "2. **Realism Medieval:** RespectÄƒ coerenÈ›a locurilor (cetÄƒÈ›i, sate, codri, mÄƒnÄƒstiri, drumuri de negoÈ›) È™i a personajelor (boieri, cÄƒlÄƒreÈ›i ai curÈ›ii, È›Äƒrani, monahi, negustori). "
    "3. **Firul Narativ:** Povestea se leagÄƒ de isprÄƒvi domneÈ™ti, slujbe trimise de Vlad VodÄƒ, sau Ã®ntÃ¢lniri ce dezvÄƒluie secrete È™i primejdii ale vremii (ex: atacuri otomane, comploturi boiereÈ™ti, legende locale). "
    "4. **Descriere ScenÄƒ:** PÄƒstreazÄƒ firul narativ: locaÈ›ie, obiecte gÄƒsite/pierdute, NPC-uri, starea eroului. "
    "5. **Lungime È™i Stil:** Scrie **strict 2-4 propoziÈ›ii** vii, direct legate de acÈ›iunea jucÄƒtorului, evitÃ¢nd pasajele lungi sau divagaÈ›iile. "
    "6. **OpÈ›iuni:** OferÄƒ **mereu 2-3 opÈ›iuni clare** de acÈ›iune jucÄƒtorului la final. FÄƒ-le concise È™i distincte."
)


@st.cache_resource(show_spinner=True)
def load_local_model():
    try:
        model_name = "distilgpt2"
        cache_dir = os.getenv("HF_HOME", None)
        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, torch_dtype=torch.float32, device_map="cpu")
        return tokenizer, model
    except Exception as e:
        return None, None

def get_groq_token():
    token = os.getenv("GROQ_API_KEY")
    if token: return token
    try:
        if "GROQ_API_KEY" in st.secrets:
            token = st.secrets["GROQ_API_KEY"]
            os.environ["GROQ_API_KEY"] = token
            return token
    except: pass
    return None

def validate_groq_token():
    token = get_groq_token()
    if not token:
        st.error("ğŸ”‘ **GROQ_API_KEY lipseÈ™te!**")
        st.info("AdaugÄƒ-l Ã®n fiÈ™ierul `.env` (local) sau Ã®n `Secrets` (Cloud):")
        st.code("GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", language="bash")
        return False
    if not token.startswith("gsk_"):
        st.warning("âš ï¸ Tokenul nu pare valid (trebuie sÄƒ Ã®nceapÄƒ cu 'gsk_')")
    return True

def clean_ai_response(text: str) -> str:
    if not text: return ""
    
    # EliminÄƒm artifact-urile comune
    markers = [
        "<|im_start|>", "<|im_end|>", "user", "assistant", "system", 
        "System:", "User:", "Assistant:", "*", "[End of response]", 
        "[END]", "End of response."
    ]
    for m in markers: 
        text = text.replace(m, "")
    
    # EliminÄƒm spaÈ›ii multiple È™i newline-uri excesive
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

def fix_romanian_grammar(text: str) -> str:
    if not text or not isinstance(text, str):
        return text
    
    corrections = {
        # GreÈ™elile tale specificate
        r'\bturchi\b': 'turci',
        r'\bunui pÄƒsÄƒri\b': 'unei pÄƒsÄƒri',  # â­• FIX PENTRU PROBLEMA TA
        r'\bunei pÄƒsÄƒri\b': 'unei pÄƒsÄƒri',   # ConfirmÄƒ forma corectÄƒ
        r'\bunui (pÄƒsÄƒri|bestii|creaturi)\b': r'unor \1',  # Plural corect
        r'\bsÄƒgeatÄƒ Ã®ncoace\b': 'sÄƒgeatÄƒ din spate',
        r'\bte atacÄƒ pe tine\b': 'te atacÄƒ',
        r'\bpentru ca\b': 'pentru cÄƒ',
        r'\bsa (?!fi)\b': 'sÄƒ ',
        r'\bcu o forÈ›Äƒ mare\b': 'cu forÈ›Äƒ mare',
        # Diacritice
        r'\b(ÅŸ|Å£)\b': lambda m: 'È™' if m.group(1) == 'ÅŸ' else 'È›',
        r'\bÃ®l\b': 'Ã®l',
        r'\bÃ®i\b': 'Ã®i',
        r'\bÃ®È›i\b': 'Ã®È›i',
        # Articole + substantive
        r'\b(o|un) (armÄƒ|sÄƒgeatÄƒ|pumnal|secure|suliÈ›Äƒ)\b': r'\1n \2',
    }
    
    for pattern, replacement in corrections.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Capitalizare È™i punct final
    text = re.sub(r'\s+', ' ', text.strip())
    if text and len(text) > 1:
        text = text[0].upper() + text[1:]
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    return text

def generate_with_api(prompt: str, use_api: bool = True) -> NarrativeResponse:
    """
    GenereazÄƒ rÄƒspuns folosind Groq API È™i returneazÄƒ obiect Pydantic validat.
    ForceazÄƒ format JSON È™i aplicÄƒ corecÈ›ii automate, cu 2 retry-uri pe erori JSON.
    """
    token = get_groq_token()
    if not token:
        return NarrativeResponse(
            narrative="Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ GROQ_API_KEY Ã®n .env)",
            game_over=True
        )

    api_url = "https://api.groq.com/openai/v1/chat/completions"
    model = "openai/gpt-oss-120b"
    max_retries = 3  # Ãncercarea iniÈ›ialÄƒ + 2 reÃ®ncercÄƒri
    
    for attempt in range(max_retries):
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system", 
                    "content": (
                        "EÈ™ti un sistem JSON de joc D&D. ReturneazÄƒ EXCLUSIV JSON valid conform schemei, "
                        "fÄƒrÄƒ markdown, fÄƒrÄƒ comentarii, fÄƒrÄƒ text suplimentar. "
                        "AsigurÄƒ-te cÄƒ 'narrative' este Ã®n romÃ¢nÄƒ medievalÄƒ corectÄƒ."
                    )
                },
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.8,
            "max_tokens": 1024,
            "stream": False,
            "response_format": {"type": "json_object"}
        }

        try:
            response = requests.post(
                api_url,
                headers={
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=45
            )
            
            if response.status_code == 200:
                data = response.json()
                content = data["choices"][0]["message"]["content"].strip()
                
                content = re.sub(r'```json\s*', '', content)
                content = re.sub(r'```\s*', '', content)
                content = content.strip()
                
                # Parse JSON
                try:
                    json_data = json.loads(content)
                    
                    if "narrative" in json_data:
                        json_data["narrative"] = fix_romanian_grammar(json_data["narrative"])
                    
                    if "items_gained" in json_data and isinstance(json_data["items_gained"], list):
                        items_gained = []
                        for item_dict in json_data["items_gained"]:
                            item_dict.setdefault("type", "diverse")
                            item_dict.setdefault("value", 0)
                            item_dict.setdefault("quantity", 1)
                            items_gained.append(InventoryItem(**item_dict))
                        json_data["items_gained"] = items_gained
                    
                    print(f"\n{'='*40} LLM RAW RESPONSE {'='*40}")
                    print(f"JSON RAW Content: {content}") 
                    print(f"CÃ¢mp 'suggestions' existÄƒ: {'suggestions' in json_data}")
                    if 'suggestions' in json_data:
                        print(f"Valoare sugestii: {json_data['suggestions']}")
                    print(f"{'='*90}\n")
                    
                    # ğŸ”¥ ValideazÄƒ È™i returneazÄƒ Pydantic model - SUCCESS EXIT POINT
                    return NarrativeResponse(**json_data)
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON Decode Error: {e} - ReÃ®ncercare {attempt + 1}/{max_retries - 1}...")
                    time.sleep(1) # PauzÄƒ scurtÄƒ Ã®nainte de reÃ®ncercare
                    continue # Mergi la urmÄƒtoarea Ã®ncercare
                    
                except ValidationError as e:
                    print(f"âŒ Pydantic Validation Error: {e} - ReÃ®ncercare {attempt + 1}/{max_retries - 1}...")
                    print(f"ğŸ“„ JSON data (Validation Failed): {json_data}") 
                    time.sleep(1) # PauzÄƒ scurtÄƒ Ã®nainte de reÃ®ncercare
                    continue # Mergi la urmÄƒtoarea Ã®ncercare

                except Exception as e:
                    print(f"âŒ Unexpected Error during Pydantic/Data processing: {e}")
                    import traceback
                    traceback.print_exc()
                    break # IeÈ™i din buclÄƒ la eroare neaÈ™teptatÄƒ
            
            # ğŸ”¥ Handle specific API errors (401, 429, 503) - These should break or return immediately
            elif response.status_code == 401:
                st.error("âŒ Token invalid! Status 401 - VerificÄƒ GROQ_API_KEY")
                return NarrativeResponse(narrative="Autentificare eÈ™uatÄƒ. Token-ul API este invalid sau expirat.", game_over=True)
            elif response.status_code == 429:
                st.warning("âš ï¸ Rate limit atins.")
                if attempt < max_retries - 1:
                    time.sleep(random.randint(2, 5))
                    continue
                else:
                    return NarrativeResponse(narrative="API-ul este ocupat.", game_over=False)
            elif response.status_code == 503:
                print(f"âš ï¸ Service Unavailable (503): {model}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                else:
                    return NarrativeResponse(narrative="Serviciul este temporar indisponibil.", game_over=False)
            else:
                print(f"âš ï¸ Unexpected status code: {response.status_code}")
                break # IeÈ™i din buclÄƒ la alte erori HTTP
        
        except requests.exceptions.Timeout:
            print(f"â±ï¸ Timeout la {model} (45s)")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            else:
                return NarrativeResponse(narrative="Cererea a expirat. VerificÄƒ conexiunea la internet.", game_over=False)
        except Exception as e:
            print(f"âŒ ExcepÈ›ie neaÈ™teptatÄƒ Ã®n REQUEST: {e}")
            import traceback
            traceback.print_exc()
            break # IeÈ™i din buclÄƒ la erori de reÈ›ea sau altele
            
    # DacÄƒ bucla s-a terminat fÄƒrÄƒ succes (din cauza JSON/Pydantic errors sau break)
    return NarrativeResponse(
        narrative="Naratorul este epuizat. Nu a putut genera un rÄƒspuns valid dupÄƒ multiple Ã®ncercÄƒri.",
        game_over=False
    )
    
def generate_narrative_with_progress(prompt: str, use_api: bool = True) -> NarrativeResponse:
    """
    GenereazÄƒ narativ cu barÄƒ de progres animatÄƒ È™i returneazÄƒ NarrativeResponse.
    PÄƒstreazÄƒ experienÈ›a "Scribii lui Vlad scriu..." Ã®n timp ce API-ul lucreazÄƒ.
    """
    result_container = {"response": None, "error": None}
    
    def run_gen():
        try:
            result_container["response"] = generate_with_api(prompt, use_api)
        except Exception as e:
            result_container["error"] = str(e)
            print(f"âŒ Eroare Ã®n thread-ul de generare: {e}")
    
    # PorneÈ™te thread-ul de generare Ã®n background
    t = threading.Thread(target=run_gen, daemon=True)
    add_script_run_ctx(t)
    t.start()
    
    # ğŸ”¥ UI de progres - animaÈ›ia vizibilÄƒ
    progress_container = st.empty()
    status_text = st.empty()
    
    with progress_container:
        progress_bar = st.progress(0)
        progress = 0
        
        # ActualizeazÄƒ progresul cÃ¢t timp thread-ul ruleazÄƒ
        while t.is_alive():
            time.sleep(0.1)
            if progress < 85:
                progress = min(progress + random.randint(1, 5), 85)
                progress_bar.progress(progress)
                status_text.markdown(
                    f'<div class="progress-text">âš”ï¸ Scribii lui Vlad scriu... {progress}%</div>',
                    unsafe_allow_html=True
                )
        
        # AÈ™teaptÄƒ finalizarea thread-ului
        t.join()
        
        # CompleteazÄƒ animaÈ›ia la 100%
        for i in range(progress, 101):
            progress_bar.progress(i)
            time.sleep(0.005)
        
        # CurÄƒÈ›Äƒ textul de status
        status_text.empty()
    
    # CurÄƒÈ›Äƒ containerul de progres
    progress_container.empty()
    
    # ğŸ”¥ ProceseazÄƒ rezultatul
    if result_container["error"]:
        st.error(f"ğŸ§™ NARATOR: **Eroare CriticÄƒ**: {result_container['error']}")
        return NarrativeResponse(
            narrative="Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)",
            game_over=True
        )
    
    response = result_container["response"]
    if not response:
        return NarrativeResponse(
            narrative="Nu am putut genera un rÄƒspuns valid.",
            game_over=False
        )
    
    return response

def generate_local(prompt: str) -> str:
    tokenizer, model = load_local_model()
    if not tokenizer or not model:
        st.error("âŒ Modelul local nu este disponibil. InstaleazÄƒ `distilgpt2` manual.")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    try:
        context_prompt = f"Fantasy story: {prompt}"
        inputs = tokenizer(context_prompt, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=80, do_sample=True, temperature=0.9, pad_token_id=tokenizer.eos_token_id)
        text = tokenizer.decode(out[0], skip_special_tokens=True)
        result = clean_ai_response(text.replace(context_prompt, ""))
        return result
    except Exception as e:
        st.error(f"âŒ Eroare la generarea localÄƒ: {e}")
        return "Ceva a tulburat liniÈ™tea..."

def generate_story_text(prompt: str, use_api: bool = True) -> str:
    if use_api:
        if validate_groq_token():
            res = generate_with_api(prompt)
            if res != "api_fail": return res
            st.warning("âš ï¸ API a eÈ™uat complet. Folosesc modelul local...")
        else:
            st.warning("âš ï¸ Token invalid. Folosesc modelul local...")
    return generate_local(prompt)

def generate_story_text_with_progress(prompt: str, use_api: bool = True) -> str:
    result_container = {"text": "", "done": False, "error": None}
    def run_gen():
        try:
            result_container["text"] = generate_story_text(prompt, use_api)
        except Exception as e:
            result_container["error"] = str(e)
        finally:
            result_container["done"] = True
    t = threading.Thread(target=run_gen)
    add_script_run_ctx(t)
    t.start()
    progress_container = st.empty()
    status_text = st.empty()
    with progress_container:
        progress_bar = st.progress(0)
        progress = 0
        while t.is_alive():
            time.sleep(0.1)
            if progress < 85:
                progress = min(progress + random.randint(1, 5), 85)
                progress_bar.progress(progress)
                status_text.markdown(f'<div class="progress-text">âš”ï¸ Scribii lui Vlad scriu... {progress}%</div>', unsafe_allow_html=True)
        t.join()
        for i in range(progress, 101):
            progress_bar.progress(i)
            time.sleep(0.005)
        status_text.empty()
    progress_container.empty()
    if result_container["error"]:
        st.error(f"ğŸ§™ NARATOR: **CRITICAL ERROR**: {result_container['error']}")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    final = result_container["text"]
    if not final or final in ["api_fail", ""]:
        st.error("ğŸ§™ NARATOR: **CRITICAL ERROR**: No model available. Check GROQ_API_KEY and internet.")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    return final