import os
import sys
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import requests
import threading
from typing import List, Optional
import time
import random
import re
import json
from streamlit.runtime.scriptrunner import add_script_run_ctx
from pydantic import ValidationError # â­• FIX: Added explicit Pydantic ValidationError import

from models import InventoryItem, NarrativeResponse

if os.name == 'nt':
    os.environ["HF_HOME"] = "D:/huggingface_cache"
    os.makedirs("D:/huggingface_cache", exist_ok=True)


# Thread-safe rotation pentru Groq API keys
_groq_key_index = 0
_groq_key_lock = threading.Lock()

# llm_handler.py
SYSTEM_PROMPT = (
    "EÈ™ti Naratorul TÄƒrÃ¢mului Valah Ã®n veacul al XV-lea, Ã®n zilele domniei lui Vlad ÈšepeÈ™ (DrÄƒculea). "
    "Tonul tÄƒu este medieval romÃ¢nesc: grav, aspru, veridic È™i autentic, folosind expresii arhaice È™i un vocabular variat specific epocii. "
    "DIALOG DIRECT & FORMAL: CÃ¢nd adresez o Ã®ntrebare unui personaj, mai ales NPC-uri majore ca Vlad ÈšepeÈ™, favorizeazÄƒ dialogul Ã®n locul naraÈ›iunii È™i oferÄƒ prioritar replica Ã®n **GHILIMELE** duble (\"\") alÄƒturi de contextul naratorului."
    "Nu folosi obiecte, noÈ›iuni sau emoÈ›ii moderne (ex: puÈ™ti, singurÄƒtate, fricÄƒ excesivÄƒ) È™i evitÄƒ orice meta-comentariu. "

    "\n\n**MECANICA NARATIVÄ‚ È˜I DIALOGUL:**"
    "1. **Anti-RepetiÈ›ie StrictÄƒ:** VarieazÄƒ structura propoziÈ›iilor, descrierile (vÃ¢nt/umbre) È™i verbele. Nu repeta descrieri similare Ã®n douÄƒ rÄƒspunsuri consecutive. "
    "2. **Realism Medieval:** RespectÄƒ coerenÈ›a locurilor (cetÄƒÈ›i, sate, codri, mÄƒnÄƒstiri, drumuri de negoÈ›) È™i a personajelor (boieri, cÄƒlÄƒreÈ›i ai curÈ›ii, È›Äƒrani, monahi, negustori). "
    "3. **Firul Narativ:** Povestea se leagÄƒ de isprÄƒvi domneÈ™ti, slujbe trimise de Vlad VodÄƒ, sau Ã®ntÃ¢lniri ce dezvÄƒluie secrete È™i primejdii ale vremii (ex: atacuri otomane, comploturi boiereÈ™ti, legende locale). "
    "4. **Descriere ScenÄƒ:** PÄƒstreazÄƒ firul narativ: locaÈ›ie, obiecte gÄƒsite/pierdute, NPC-uri, starea eroului. "
    "5. **Lungime È™i Stil:** Scrie strict 2-4 propoziÈ›ii vii, direct legate de acÈ›iunea jucÄƒtorului, evitÃ¢nd pasajele lungi sau divagaÈ›iile. "
    "6. **OpÈ›iuni (FÄ‚RÄ‚ REPETIÈšIE):** OferÄƒ **mereu 2-3 opÈ›iuni clare** de acÈ›iune jucÄƒtorului la final. **Nu repeta aceleaÈ™i opÈ›iuni** dacÄƒ nu au fost alese, ci continuÄƒ logic firul narativ." # <--- ADÄ‚UGATÄ‚ REGULÄ‚ ANTI-REPETIÈšIE AICI
)

def get_session_id():
    """ObÈ›ine ID-ul de sesiune din Streamlit session_state"""
    return st.session_state.get('session_id', 'UNKNOWN_SESSION')

def get_all_groq_tokens() -> List[str]:
    """ObÈ›ine TOATE cheile Groq din mediu: GROQ_API_KEY, GROQ_API_KEY1, GROQ_API_KEY2, etc."""
    tokens = []
    # Cheia principalÄƒ
    token = os.getenv("GROQ_API_KEY")
    if token and token.strip():
        tokens.append(token.strip())
    
    # Chei secundare (GROQ_API_KEY1, GROQ_API_KEY2, ...)
    i = 1
    while True:
        token = os.getenv(f"GROQ_API_KEY{i}")
        if token and token.strip():
            tokens.append(token.strip())
            i += 1
        else:
            break
    
    # EliminÄƒ duplicate pÄƒstrÃ¢nd ordinea
    seen = set()
    unique_tokens = []
    for token in tokens:
        if token not in seen:
            seen.add(token)
            unique_tokens.append(token)
    
    return unique_tokens


@st.cache_resource(show_spinner=True)
def load_local_model():
    try:
        model_name = "distilgpt2"
        cache_dir = os.getenv("HF_HOME", None)
        tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, torch_dtype=torch.float32, device_map="cpu")
        return tokenizer, model
    except Exception as e:
        return None, None

def get_groq_token():
    token = os.getenv("GROQ_API_KEY")
    if token: return token
    try:
        if "GROQ_API_KEY" in st.secrets:
            token = st.secrets["GROQ_API_KEY"]
            os.environ["GROQ_API_KEY"] = token
            return token
    except: pass
    return None

def validate_groq_token():
    token = get_groq_token()
    if not token:
        st.error("ğŸ”‘ **GROQ_API_KEY lipseÈ™te!**")
        st.info("AdaugÄƒ-l Ã®n fiÈ™ierul `.env` (local) sau Ã®n `Secrets` (Cloud):")
        st.code("GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", language="bash")
        return False
    if not token.startswith("gsk_"):
        st.warning("âš ï¸ Tokenul nu pare valid (trebuie sÄƒ Ã®nceapÄƒ cu 'gsk_')")
    return True

def clean_ai_response(text: str) -> str:
    if not text: return ""
    
    # EliminÄƒm artifact-urile comune
    markers = [
        "<|im_start|>", "<|im_end|>", "user", "assistant", "system", 
        "System:", "User:", "Assistant:", "*", "[End of response]", 
        "[END]", "End of response."
    ]
    for m in markers: 
        text = text.replace(m, "")
    
    # EliminÄƒm spaÈ›ii multiple È™i newline-uri excesive
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

def fix_romanian_grammar(text: str) -> str:
    if not text or not isinstance(text, str):
        return text
    
    corrections = {
        # GreÈ™elile tale specificate
        r'\bturchi\b': 'turci',
        r'\bunui pÄƒsÄƒri\b': 'unei pÄƒsÄƒri',  # â­• FIX PENTRU PROBLEMA TA
        r'\bunei pÄƒsÄƒri\b': 'unei pÄƒsÄƒri',   # ConfirmÄƒ forma corectÄƒ
        r'\bunui (pÄƒsÄƒri|bestii|creaturi)\b': r'unor \1',  # Plural corect
        r'\bsÄƒgeatÄƒ Ã®ncoace\b': 'sÄƒgeatÄƒ din spate',
        r'\bte atacÄƒ pe tine\b': 'te atacÄƒ',
        r'\bpentru ca\b': 'pentru cÄƒ',
        r'\bsa (?!fi)\b': 'sÄƒ ',
        r'\bcu o forÈ›Äƒ mare\b': 'cu forÈ›Äƒ mare',
        # Diacritice
        r'\b(ÅŸ|Å£)\b': lambda m: 'È™' if m.group(1) == 'ÅŸ' else 'È›',
        r'\bÃ®l\b': 'Ã®l',
        r'\bÃ®i\b': 'Ã®i',
        r'\bÃ®È›i\b': 'Ã®È›i',
        # Articole + substantive
        r'\b(o|un) (armÄƒ|sÄƒgeatÄƒ|pumnal|secure|suliÈ›Äƒ)\b': r'\1n \2',
    }
    
    for pattern, replacement in corrections.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Capitalizare È™i punct final
    text = re.sub(r'\s+', ' ', text.strip())
    if text and len(text) > 1:
        text = text[0].upper() + text[1:]
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    return text

def generate_with_api(prompt: str, use_api: bool = True) -> NarrativeResponse:
    """
    GenereazÄƒ rÄƒspuns folosind Groq API cu rotaÈ›ie inteligentÄƒ de chei.
    La fiecare request se roteÈ™te la urmÄƒtoarea cheie. DacÄƒ o cheie eÈ™ueazÄƒ,
    se Ã®ncearcÄƒ automat urmÄƒtoarea din listÄƒ.
    """
    session_id = get_session_id()  # â­• OBTINE ID SESIUNE
    tokens = get_all_groq_tokens()
    if not tokens:
        print(f"[SESSION {session_id}] ğŸ”‘ NO GROQ TOKENS FOUND")  # â­• LOG
        return NarrativeResponse(
            narrative="Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ GROQ_API_KEY Ã®n .env)",
            game_over=True
        )
    print(f"[SESSION {session_id}] ğŸ”‘ USING TOKEN: {tokens[start_index][:10]}...")  # â­• LOG TOKEN
    api_url = "https://api.groq.com/openai/v1/chat/completions"
    model = "openai/gpt-oss-120b"
    max_retries_per_key = 1  # Doar 1 Ã®ncercare per cheie Ã®nainte de a roti
    
    # Thread-safe rotation: determinÄƒm cheia de start pentru acest request
    global _groq_key_index
    with _groq_key_lock:
        start_index = _groq_key_index
        # IncrementÄƒm pentru urmÄƒtorul request
        _groq_key_index = (_groq_key_index + 1) % len(tokens)
    
    # ÃncercÄƒm fiecare cheie Ã®ncepÃ¢nd de la index-ul rotit
    for i in range(len(tokens)):
        token_index = (start_index + i) % len(tokens)
        token = tokens[token_index]
        
        # AfiÈ™Äƒm doar dacÄƒ avem mai multe chei
        if len(tokens) > 1:
            st.toast(f"ğŸ”‘ Folosind cheia Groq {token_index + 1}/{len(tokens)}", icon="ğŸ”„")
        
        for attempt in range(max_retries_per_key):
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "system", 
                        "content": SYSTEM_PROMPT
                    },
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.8,
                "max_tokens": 1024,
                "stream": False,
                "response_format": {"type": "json_object"}
            }

            try:
                response = requests.post(
                    api_url,
                    headers={
                        "Authorization": f"Bearer {token}",
                        "Content-Type": "application/json"
                    },
                    json=payload,
                    timeout=45
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    content = re.sub(r'```json\s*', '', content)
                    content = re.sub(r'```\s*', '', content)
                    content = content.strip()
                    
                    try:
                        json_data = json.loads(content)
                        
                        if "narrative" in json_data:
                            json_data["narrative"] = fix_romanian_grammar(json_data["narrative"])
                        
                        if "items_gained" in json_data and isinstance(json_data["items_gained"], list):
                            items_gained = []
                            for item_dict in json_data["items_gained"]:
                                item_dict.setdefault("type", "diverse")
                                item_dict.setdefault("value", 0)
                                item_dict.setdefault("quantity", 1)
                                items_gained.append(InventoryItem(**item_dict))
                            json_data["items_gained"] = items_gained
                        
                        #print(f"\n{'='*40} LLM RAW RESPONSE {'='*40}")
                        #print(f"JSON RAW Content: {content}") 
                        #rint(f"CÃ¢mp 'suggestions' existÄƒ: {'suggestions' in json_data}")
                        #if 'suggestions' in json_data:
                        #    print(f"Valoare sugestii: {json_data['suggestions']}")
                        #print(f"{'='*90}\n")
                        print(f"[SESSION {session_id}] âœ… SUCCESS WITH TOKEN {token_index + 1}")  # â­• LOG SUCCES
                        # ReturnÄƒm rÄƒspunsul validat
                        return NarrativeResponse(**json_data)
                        
                    except json.JSONDecodeError as e:
                        print(f"[SESSION {session_id}] âŒ TOKEN {token_index + 1} JSON Decode Error: {e}")  # â­• LOG
                        
                        if attempt < max_retries_per_key - 1:
                            time.sleep(1)
                            continue
                        else:
                            st.warning(f"âš ï¸ JSON invalid cu cheia {token_index + 1}, trecem la urmÄƒtoarea...")
                            break
                            
                    except ValidationError as e:
                        print(f"[SESSION {session_id}] âŒ TOKEN {token_index + 1} Pydantic Validation Error: {e} {json_data}")  # â­• LOG
                        if attempt < max_retries_per_key - 1:
                            time.sleep(1)
                            continue
                        else:
                            st.warning(f"âš ï¸ Validare eÈ™uatÄƒ cu cheia {token_index + 1}, trecem la urmÄƒtoarea...")
                            break

                    except Exception as e:
                        print(f"[SESSION {session_id}] âŒ TOKEN {token_index + 1} Unexpected Error during Pydantic/Data processing: {e}")  # â­• LOG
                        import traceback
                        traceback.print_exc()
                        break
                
                # Handle specific API errors
                elif response.status_code == 401:
                    print(f"[SESSION {session_id}] âŒ TOKEN {token_index + 1} INVALID (401)")  # â­• LOG
                    st.error(f"âŒ Cheia {token_index + 1} este invalidÄƒ (401)!")
                    break  # Trecem la urmÄƒtoarea cheie
                elif response.status_code == 429:
                    print(f"[SESSION {session_id}] âš ï¸ TOKEN {token_index + 1} RATE LIMITED (429)")  # â­• LOG
                    st.warning(f"âš ï¸ Rate limit atins pentru cheia {token_index + 1} (429).")
                    break  # Trecem la urmÄƒtoarea cheie
                elif response.status_code == 503:
                    print(f"[SESSION {session_id}] âš ï¸ TOKEN {token_index + 1} Service Unavailable (503): {model}")  # â­• LOG
                    break  # Trecem la urmÄƒtoarea cheie
                else:
                    print(f"[SESSION {session_id}] âš ï¸ TOKEN {token_index + 1} Unexpected status code: {model}")  # â­• LOG
                    break
            
            except requests.exceptions.Timeout:
                print(f"[SESSION {session_id}] â±ï¸ TIMEOUT TOKEN {token_index + 1}")  # â­• LOG
                break  # Trecem la urmÄƒtoarea cheie
            except Exception as e:
                print(f"[SESSION {session_id}] âŒ Unknown EXCEPTION TOKEN {token_index + 1}: {e}")  # â­• LOG
                import traceback
                traceback.print_exc()
                break
    print(f"[SESSION {session_id}] âŒ ALL TOKENS FAILED")  # â­• LOG
    # DacÄƒ am epuizat toate cheile
    return NarrativeResponse(
        narrative=f"Toate conexiunile magice au eÈ™uat. (VerificÄƒ {len(tokens)} GROQ_API_KEY Ã®n .env)",
        game_over=True
    )

    
def generate_narrative_with_progress(prompt: str, use_api: bool = True) -> NarrativeResponse:
    """
    GenereazÄƒ narativ cu barÄƒ de progres animatÄƒ È™i returneazÄƒ NarrativeResponse.
    PÄƒstreazÄƒ experienÈ›a "Scribii lui Vlad scriu..." Ã®n timp ce API-ul lucreazÄƒ.
    """
    result_container = {"response": None, "error": None}
    
    def run_gen():
        try:
            result_container["response"] = generate_with_api(prompt, use_api)
        except Exception as e:
            result_container["error"] = str(e)
            print(f"âŒ Eroare Ã®n thread-ul de generare: {e}")
    
    # PorneÈ™te thread-ul de generare Ã®n background
    t = threading.Thread(target=run_gen, daemon=True)
    add_script_run_ctx(t)
    t.start()
    
    # ğŸ”¥ UI de progres - animaÈ›ia vizibilÄƒ
    progress_container = st.empty()
    status_text = st.empty()
    
    with progress_container:
        progress_bar = st.progress(0)
        progress = 0
        
        # ActualizeazÄƒ progresul cÃ¢t timp thread-ul ruleazÄƒ
        while t.is_alive():
            time.sleep(0.1)
            if progress < 85:
                progress = min(progress + random.randint(1, 5), 85)
                progress_bar.progress(progress)
                status_text.markdown(
                    f'<div class="progress-text">âš”ï¸ Scribii lui Vlad scriu... {progress}%</div>',
                    unsafe_allow_html=True
                )
        
        # AÈ™teaptÄƒ finalizarea thread-ului
        t.join()
        
        # CompleteazÄƒ animaÈ›ia la 100%
        for i in range(progress, 101):
            progress_bar.progress(i)
            time.sleep(0.005)
        
        # CurÄƒÈ›Äƒ textul de status
        status_text.empty()
    
    # CurÄƒÈ›Äƒ containerul de progres
    progress_container.empty()
    
    # ğŸ”¥ ProceseazÄƒ rezultatul
    if result_container["error"]:
        st.error(f"ğŸ§™ NARATOR: **Eroare CriticÄƒ**: {result_container['error']}")
        return NarrativeResponse(
            narrative="Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)",
            game_over=True
        )
    
    response = result_container["response"]
    if not response:
        return NarrativeResponse(
            narrative="Nu am putut genera un rÄƒspuns valid.",
            game_over=False
        )
    
    return response

def generate_local(prompt: str) -> str:
    tokenizer, model = load_local_model()
    if not tokenizer or not model:
        st.error("âŒ Modelul local nu este disponibil. InstaleazÄƒ `distilgpt2` manual.")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    try:
        context_prompt = f"Fantasy story: {prompt}"
        inputs = tokenizer(context_prompt, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=80, do_sample=True, temperature=0.9, pad_token_id=tokenizer.eos_token_id)
        text = tokenizer.decode(out[0], skip_special_tokens=True)
        result = clean_ai_response(text.replace(context_prompt, ""))
        return result
    except Exception as e:
        st.error(f"âŒ Eroare la generarea localÄƒ: {e}")
        return "Ceva a tulburat liniÈ™tea..."

def generate_story_text(prompt: str, use_api: bool = True) -> str:
    if use_api:
        if validate_groq_token():
            res = generate_with_api(prompt)
            if res != "api_fail": return res
            st.warning("âš ï¸ API a eÈ™uat complet. Folosesc modelul local...")
        else:
            st.warning("âš ï¸ Token invalid. Folosesc modelul local...")
    return generate_local(prompt)

def generate_story_text_with_progress(prompt: str, use_api: bool = True) -> str:
    result_container = {"text": "", "done": False, "error": None}
    def run_gen():
        try:
            result_container["text"] = generate_story_text(prompt, use_api)
        except Exception as e:
            result_container["error"] = str(e)
        finally:
            result_container["done"] = True
    t = threading.Thread(target=run_gen)
    add_script_run_ctx(t)
    t.start()
    progress_container = st.empty()
    status_text = st.empty()
    with progress_container:
        progress_bar = st.progress(0)
        progress = 0
        while t.is_alive():
            time.sleep(0.1)
            if progress < 85:
                progress = min(progress + random.randint(1, 5), 85)
                progress_bar.progress(progress)
                status_text.markdown(f'<div class="progress-text">âš”ï¸ Scribii lui Vlad scriu... {progress}%</div>', unsafe_allow_html=True)
        t.join()
        for i in range(progress, 101):
            progress_bar.progress(i)
            time.sleep(0.005)
        status_text.empty()
    progress_container.empty()
    if result_container["error"]:
        st.error(f"ğŸ§™ NARATOR: **CRITICAL ERROR**: {result_container['error']}")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    final = result_container["text"]
    if not final or final in ["api_fail", ""]:
        st.error("ğŸ§™ NARATOR: **CRITICAL ERROR**: No model available. Check GROQ_API_KEY and internet.")
        return "Conexiunea cu tÄƒrÃ¢mul magic s-a Ã®ntrerupt. (VerificÄƒ Token-ul)"
    return final